{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Venmo Transactions with Spark:\n",
    "\n",
    "### Goal: \n",
    "Segment Users into groups based on the way they use Venmo most regularly for lead generation (think Cash App, Zelle, Wells-Fargo, and other money-exchange applications) and advertising targeting based solely on their transaction notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing Necessary Tools for Spark\n",
    "\n",
    "Running Spark through a jupyter notebook requires some tooling. I've installed spark, scala, and hadoop on my local machine based on the following instructions:\n",
    "https://medium.com/big-data-engineering/how-to-install-apache-spark-2-x-in-your-pc-e2047246ffc3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Spark Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.140872\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 1000000\n",
    "\n",
    "\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x * x + y * y < 1\n",
    "\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1590950446519'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.host', 'vm-cbi-12oe3f'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.driver.port', '63769'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'venmo_transactions')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "sc = pyspark.SparkContext(appName=\"venmo_transactions\").getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = 'C:\\\\Users\\\\Stuart\\\\Documents\\\\GitHub\\\\venmo\\\\data\\\\output\\\\transactions_full.csv'\n",
    "transactions_df = spark.read.csv(csv, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying Data Integrity\n",
    "\n",
    "It is smart to check the integrity of the data when working with any dataset. Here, I check the first row, total length, datatypes, and the number of null values per field in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Row : \n",
      "\n",
      " Row(transaction_id=2540405007077868184, actor_user_id=2482900494712832556, target_user_id=None, target_type='user', overall_type='payment', transaction_note=\"b'fuk ya'\", date_created='2018-08-07 02:11:16')\n",
      "\n",
      "\n",
      "Length of dataframe: \n",
      "\n",
      " 7076584 records\n"
     ]
    }
   ],
   "source": [
    "print('First Row : \\n\\n', transactions_df.first())\n",
    "\n",
    "print('\\n\\nLength of dataframe: \\n\\n', transactions_df.count(), 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: long (nullable = true)\n",
      " |-- actor_user_id: long (nullable = true)\n",
      " |-- target_user_id: string (nullable = true)\n",
      " |-- target_type: string (nullable = true)\n",
      " |-- overall_type: string (nullable = true)\n",
      " |-- transaction_note: string (nullable = true)\n",
      " |-- date_created: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+--------------+-----------+------------+----------------+------------+\n",
      "|transaction_id|actor_user_id|target_user_id|target_type|overall_type|transaction_note|date_created|\n",
      "+--------------+-------------+--------------+-----------+------------+----------------+------------+\n",
      "|             0|            0|       7076584|          0|           0|               0|          43|\n",
      "+--------------+-------------+--------------+-----------+------------+----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check for null values\n",
    "from pyspark.sql.functions import col,sum\n",
    "\n",
    "#show sum of null values after converting isnull T/F results to ints\n",
    "transactions_df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in transactions_df.columns)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating Data types, Filtering NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert datetime from string to timestamp type\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "transactions_df = transactions_df.withColumn(\"datetime_created\",transactions_df['date_created'].cast(TimestampType()))\n",
    "\n",
    "#verify conversion\n",
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7076541"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter out null rows from date_created field\n",
    "transactions_df = transactions_df.filter(transactions_df.date_created.isNotNull())\n",
    "\n",
    "#check length\n",
    "transactions_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the Data\n",
    "\n",
    "In order to model topics, the transaction notes need to be processed. Similar to many NLP approaches, I tokenize the documents, remove stopwords, and stem them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.functions import udf, regexp_replace, trim\n",
    "from pyspark.sql.types import ArrayType, StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|     transaction_id|         spaced_note|           note_no_b|        trimmed_note|         note_tokens| note_sans_stopwords|        note_stemmed|       date_created|\n",
      "+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|2540405007077868184|           b'fuk ya'|             fuk ya'|              fuk ya|           [fuk, ya]|           [fuk, ya]|           [fuk, ya]|2018-08-07 02:11:16|\n",
      "|2540405006884930468|     b' automobile '|        automobile '|          automobile|        [automobile]|        [automobile]|         [automobil]|2018-08-07 02:11:16|\n",
      "|2540405007379857710|   b' venmo_dollar '|      venmo_dollar '|        venmo_dollar|      [venmo_dollar]|      [venmo_dollar]|      [venmo_dollar]|2018-08-07 02:11:16|\n",
      "|2540404998227886310|         b'Gatorade'|           Gatorade'|            Gatorade|          [gatorade]|          [gatorade]|           [gatorad]|2018-08-07 02:11:15|\n",
      "|2540404998613762676|   b' party_popper '|      party_popper '|        party_popper|      [party_popper]|      [party_popper]|        [party_popp]|2018-08-07 02:11:15|\n",
      "|2540404999217741947|            b'China'|              China'|               China|             [china]|             [china]|             [china]|2018-08-07 02:11:15|\n",
      "|2540404999972717093|             b'Boyz'|               Boyz'|                Boyz|              [boyz]|              [boyz]|              [boyz]|2018-08-07 02:11:15|\n",
      "|2540404999746224995|              b'Tab'|                  Ta|                  Ta|                [ta]|                [ta]|                [ta]|2018-08-07 02:11:15|\n",
      "|2540404998253052168|    b' cut_of_meat '|       cut_of_meat '|         cut_of_meat|       [cut_of_meat]|       [cut_of_meat]|       [cut_of_meat]|2018-08-07 02:11:15|\n",
      "|2540405001474278289|b' dumpling  dump...| dumpling  dumpli...|dumpling dumpling...|[dumpling, dumpli...|[dumpling, dumpli...|[dumpl, dumpl, du...|2018-08-07 02:11:15|\n",
      "|2540405008579429281|         b'Internet'|           Internet'|            Internet|          [internet]|          [internet]|          [internet]|2018-08-07 02:11:16|\n",
      "|2540405001952428161|b'Top  flag_in_ho...|Top  flag_in_hole...|Top flag_in_hole ...|[top, flag_in_hol...|[top, flag_in_hol...|[top, flag_in_hol...|2018-08-07 02:11:15|\n",
      "|2540405002388636555|   b' hugging_face '|      hugging_face '|        hugging_face|      [hugging_face]|      [hugging_face]|       [hugging_fac]|2018-08-07 02:11:15|\n",
      "|2540405004745834978|            b'Pizza'|              Pizza'|               Pizza|             [pizza]|             [pizza]|             [pizza]|2018-08-07 02:11:15|\n",
      "|2540405002036314887|b' taxi  taxi  ta...|  taxi  taxi  taxi '|      taxi taxi taxi|  [taxi, taxi, taxi]|  [taxi, taxi, taxi]|  [taxi, taxi, taxi]|2018-08-07 02:11:15|\n",
      "|2540405008780755050|         b' dragon '|            dragon '|              dragon|            [dragon]|            [dragon]|            [dragon]|2018-08-07 02:11:16|\n",
      "|2540405008067723799|b' face_with_symb...| face_with_symbol...|face_with_symbols...|[face_with_symbol...|[face_with_symbol...|[face_with_symbol...|2018-08-07 02:11:16|\n",
      "|2540405009024024589|            b'shirt'|              shirt'|               shirt|             [shirt]|             [shirt]|             [shirt]|2018-08-07 02:11:16|\n",
      "|2540405003688870875|                b'.'|                  .'|                   .|                 [.]|                 [.]|                 [.]|2018-08-07 02:11:15|\n",
      "|2540404990627808127|              b'MTB'|                MTB'|                 MTB|               [mtb]|               [mtb]|               [mtb]|2018-08-07 02:11:14|\n",
      "+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#replace emoji colons\n",
    "emoji_spaced_df = transactions_df.withColumn(\n",
    "    'spaced_note', regexp_replace('transaction_note', ':', ' '))\n",
    "\n",
    "#replace \"b'\" from note strings, an artifact of exporting at prior stage\n",
    "dropped_b_df = emoji_spaced_df.withColumn(\n",
    "    'note_no_b', regexp_replace('spaced_note', \"b'\", ''))\n",
    "\n",
    "#drop double spaces\n",
    "single_sp_df = dropped_b_df.withColumn('note_ss',\n",
    "                                       regexp_replace('note_no_b', '  ', ' '))\n",
    "\n",
    "#drop single quotes\n",
    "single_q_df = single_sp_df.withColumn('note_sq',\n",
    "                                      regexp_replace('note_ss', \"'\", ''))\n",
    "\n",
    "#trim whitespace\n",
    "trimmed_ws_df = single_q_df.withColumn('trimmed_note', trim(col('note_sq')))\n",
    "\n",
    "#tokenize the dataframe\n",
    "tokenizer = Tokenizer(inputCol='trimmed_note', outputCol='note_tokens')\n",
    "tokenized_df = tokenizer.transform(trimmed_ws_df).select(\n",
    "    'transaction_id', 'spaced_note', 'note_no_b', 'trimmed_note',\n",
    "    'note_tokens', 'date_created')\n",
    "\n",
    "# remove stop words from tokens\n",
    "remover = StopWordsRemover(inputCol='note_tokens',\n",
    "                           outputCol='note_sans_stopwords')\n",
    "df_sans_stopwords = remover.transform(tokenized_df).select(\n",
    "    'transaction_id', 'spaced_note', 'note_no_b', 'trimmed_note',\n",
    "    'note_tokens', 'note_sans_stopwords', 'date_created')\n",
    "\n",
    "# Stem text\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens],\n",
    "                  ArrayType(StringType()))\n",
    "\n",
    "df_stemmed = df_sans_stopwords.withColumn(\n",
    "    \"note_stemmed\", stemmer_udf(\"note_sans_stopwords\")).select(\n",
    "        'transaction_id', 'spaced_note', 'note_no_b', 'trimmed_note',\n",
    "        'note_tokens', 'note_sans_stopwords', 'note_stemmed', 'date_created')\n",
    "\n",
    "df_stemmed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing User Trends: Transaction Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most Frequent Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|      actor_user_id|count|\n",
      "+-------------------+-----+\n",
      "|2135126842540032910|  359|\n",
      "|1997633002405888173|  297|\n",
      "|2021366932963328676|  235|\n",
      "|2330070962143232368|  215|\n",
      "|2048934239272960558|  187|\n",
      "|2421457825038336069|  130|\n",
      "|2181423544926208031|   91|\n",
      "|1564104179318784848|   86|\n",
      "|2411768840192000987|   63|\n",
      "|1271828970471424398|   60|\n",
      "|1874509636304896473|   58|\n",
      "|1856331942199296981|   55|\n",
      "|2004155983986688049|   53|\n",
      "|2236412472590336483|   53|\n",
      "|2337822597971968392|   50|\n",
      "|2407072335396864571|   45|\n",
      "|1548828859695104235|   45|\n",
      "|1623586188034048269|   44|\n",
      "|2522200947032064794|   43|\n",
      "|2195803229650944396|   43|\n",
      "+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg = transactions_df.groupby('actor_user_id').count()\n",
    "\n",
    "agg.orderBy(['count'], ascending=0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method sort in module pyspark.sql.dataframe:\n",
      "\n",
      "sort(*cols, **kwargs) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
      "    \n",
      "    :param cols: list of :class:`Column` or column names to sort by.\n",
      "    :param ascending: boolean or list of boolean (default ``True``).\n",
      "        Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      "        If a list is specified, length of the list must equal length of the `cols`.\n",
      "    \n",
      "    >>> df.sort(df.age.desc()).collect()\n",
      "    [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "    >>> df.sort(\"age\", ascending=False).collect()\n",
      "    [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "    >>> df.orderBy(df.age.desc()).collect()\n",
      "    [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "    >>> from pyspark.sql.functions import *\n",
      "    >>> df.sort(asc(\"age\")).collect()\n",
      "    [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "    >>> df.orderBy(desc(\"age\"), \"name\").collect()\n",
      "    [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "    >>> df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect()\n",
      "    [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "    \n",
      "    .. versionadded:: 1.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(agg.orderBy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing User Trends: Word Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmenting Users Based on Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing Each Segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
